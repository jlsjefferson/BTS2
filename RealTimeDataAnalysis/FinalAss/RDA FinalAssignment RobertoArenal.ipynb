{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c67c86",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d360c16",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93106fbb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = spark.read.option(\"sep\", \"\\t\").csv(\"./Assignment-2/train.data\")\n",
    "sol = spark.read.option(\"sep\", \"\\t\").csv(\"./Assignment-2/train.solution\")\n",
    "sol = sol.toDF('label')\n",
    "train = train.limit(10000)\n",
    "sol = sol.limit(10000)\n",
    "cols = spark.read.csv('./Assignment-2/feature.name', sep= '\\t', header= True)\n",
    "train = train.toDF(*cols.columns)\n",
    "train = train.withColumnRenamed(\"#followers\",\"followers\") \\\n",
    "        .withColumnRenamed(\"#favorites\",\"favorites\") \\\n",
    "        .withColumnRenamed(\"#friends\",\"friends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfc8ed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190bb816",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Changing data types\n",
    "train = train.drop('tweet_id')\n",
    "columns = ['followers', 'friends', 'favorites']\n",
    "train = convertColumn(train, columns, IntegerType())\n",
    "sol = convertColumn(sol, ['label'], IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd02cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Converting labels into binary classes\n",
    "sol = sol.withColumn(\"label\",F.when(F.col(\"label\")>0,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065c408",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_sol = train.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc80f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generate a sequencial id and join both dataframes. \n",
    "train_sol.createOrReplaceTempView(\"tweets\")\n",
    "train_sol = spark.sql(\"SELECT row_number() OVER (ORDER BY (SELECT NULL)) as id,* \\\n",
    "          FROM tweets\")\n",
    "sol.createOrReplaceTempView(\"sol\")\n",
    "sol = spark.sql(\"SELECT row_number() OVER (ORDER BY (SELECT NULL)) as id,* \\\n",
    "          FROM sol\")\n",
    "train_sol = train_sol.join(sol, train_sol.id == sol.id,how='left')\n",
    "train_sol = train_sol.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fbc9f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Split entities into positive and negative. \n",
    "split_col = F.split(train_sol['sentiment'], ' ')\n",
    "train_sol = train_sol.withColumn('pos', split_col.getItem(0))\n",
    "train_sol = train_sol.withColumn('neg', split_col.getItem(1))\n",
    "train_sol = train_sol.drop(\"sentiment\")\n",
    "#convert columns to integers\n",
    "columns = ['pos','neg']\n",
    "train_sol = convertColumn(train_sol, columns, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01440e5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Function that splits each entity and calculate the average of all the entities in one tweet\n",
    "def enti_score(row):\n",
    "    lis = []\n",
    "    for x in row:\n",
    "        arr=x.split(sep=\":\")\n",
    "        #print(arr[2])\n",
    "        try:\n",
    "            lis.append(float(arr[2]))\n",
    "        except:\n",
    "            continue\n",
    "    if len(lis) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return sum(lis) / len(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba9a8c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create the UDF and create a new column with the output of the UDF\n",
    "cal = F.udf(enti_score, FloatType())\n",
    "train_sol = train_sol.withColumn(\"ent_score\",cal(F.split(\"entities\",\"[;]\")))\n",
    "train_sol = train_sol.drop(\"entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0df4dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Counting mentions\n",
    "train_sol = train_sol.withColumn(\"mentions_count\",F.when(F.col(\"mentions\")!='null;',F.size(F.split(\"mentions\",\" \"))).otherwise(0))#.show()\n",
    "train_sol = train_sol.drop(\"mentions\")\n",
    "#Counting urls\n",
    "train_sol = train_sol.withColumn(\"url_count\",F.when(F.col(\"urls\")!='null;',F.size(F.split(\"urls\",\"\"\":\\-\\:\"\"\"))-1).otherwise(0))#.show()\n",
    "train_sol = train_sol.drop(\"urls\")\n",
    "#Counting hashtags\n",
    "train_sol = train_sol.withColumn(\"hash_count\",F.when(F.col(\"hashtags\")!='null;',F.size(F.split(\"hashtags\",\" \"))).otherwise(0))#.show()\n",
    "train_sol = train_sol.drop(\"hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259b897",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generate a row_number by username ordered Date\n",
    "train_sol.createOrReplaceTempView(\"tweets\")\n",
    "train_sol = spark.sql(\"SELECT *,row_number() OVER (PARTITION BY username order by Date) as Rnumb \\\n",
    "          FROM tweets\")\n",
    "\n",
    "#Get the Date from previous tweet\n",
    "train_sol.createOrReplaceTempView(\"tweets\")\n",
    "train_sol = spark.sql(\"SELECT t1.*,t2.Date as \"'PrevDate'\" \\\n",
    "          FROM tweets t1 \\\n",
    "          LEFT JOIN tweets t2 ON t1.username=t2.username and (t1.Rnumb-1)=t2.Rnumb\")\n",
    "\n",
    "#Get the days difference with previous tweet. \n",
    "train_sol = train_sol.withColumn(\"DaysSince\",F.datediff(F.col(\"Date\"),F.col(\"PrevDate\")))\n",
    "\n",
    "#First we generate a FirstTweetDate column with the date of the first tweet made by the user. \n",
    "train_sol.createOrReplaceTempView(\"tweets\")\n",
    "train_sol = spark.sql(\"SELECT t1.*,t2.Date as \"'FirstTweetDate'\"\\\n",
    "          FROM tweets t1 \\\n",
    "          LEFT JOIN tweets t2 ON t1.username=t2.username and t2.Rnumb=1\")\n",
    "\n",
    "#Second we get the difference between the tweet date and the firstTweetDate. \n",
    "train_sol = train_sol.withColumn(\"DaysSFirst\",F.datediff(F.col(\"Date\"),F.col(\"FirstTweetDate\")))\n",
    "\n",
    "#Change the 0s to 1s, so the first day of tweeting doesnt give division by 0 error. \n",
    "train_sol = train_sol.withColumn(\"DaysSFirst\",F.when(F.col(\"DaysSFirst\")==0,1).otherwise(train_sol[\"DaysSFirst\"]))\n",
    "\n",
    "#Third we divide the number of tweetsAcumm / DaysSinceFirst\n",
    "train_sol = train_sol.withColumn(\"TweetsADay\",train_sol[\"Rnumb\"]/train_sol[\"DaysSFirst\"])\n",
    "\n",
    "#Drop useless columns\n",
    "train_sol = train_sol.drop(\"PrevDate\",\"DaysSince\",\"FirstTweetDate\",\"DaysSFirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d9357",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get the Date from previous tweet\n",
    "train_sol.createOrReplaceTempView(\"tweets\")\n",
    "train_sol = spark.sql(\"SELECT t1.*,t2.label as \"'PrevRT'\" \\\n",
    "          FROM tweets t1 \\\n",
    "          LEFT JOIN tweets t2 ON t1.username=t2.username and (t1.Rnumb-1)=t2.Rnumb\")\n",
    "train_sol = train_sol.withColumn(\"PrevRT\",F.when(F.col(\"PrevRT\").isNull(),0.0).otherwise(train_sol[\"PrevRT\"]))\n",
    "\n",
    "#Drop useless columns\n",
    "train_sol = train_sol.drop(\"Rnumb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9dcc0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generate the day number of week\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "train_sol = train_sol.withColumn(\"dayNumb\",F.date_format(F.col('Date'),'u'))\n",
    "train_sol = train_sol.withColumn('dayNumb', train_sol['dayNumb'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb63e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Droping useless columns\n",
    "train_sol = train_sol.drop(\"Date\",\"username\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124f7ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_sol.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb02e2e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb4a35",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(inputCols=train_sol.columns, outputCol='vector_col')\n",
    "df_vector = assembler.transform(train_sol).select('vector_col')\n",
    "matrix = Correlation.corr(df_vector, 'vector_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fb05d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rows = matrix.collect()[0][0].toArray().tolist()\n",
    "np_array = np.array(rows)\n",
    "np_array = np.around(np_array, 4)\n",
    "df = spark.createDataFrame(np_array.tolist(),train_sol.columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453de78",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_corr_matrix(correlations,attr,fig_no):\n",
    "    f=plt.figure(fig_no, figsize=[10,10])\n",
    "    plt.matshow(correlations, fignum=f.number,vmax=1,vmin=-1)\n",
    "    plt.xticks(range(0,len(train_sol.columns)),train_sol.columns,fontsize=14,rotation=90)\n",
    "    plt.yticks(range(0,len(train_sol.columns)),train_sol.columns,fontsize=14)\n",
    "    cb = plt.colorbar(location='right',shrink=0.8)\n",
    "    cb.ax.tick_params(labelsize=12)\n",
    "    plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16e593",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----------Standard Scaling----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ec8ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=train_sol.drop(\"label\").columns, outputCol='features')\n",
    "inputDF = assembler.transform(train_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a92952",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "scaler = standardScaler.fit(inputDF)\n",
    "train_sol = scaler.transform(inputDF)\n",
    "#Replace the previous feature column with the scaled feature. \n",
    "train_sol = train_sol.drop(\"features\")\n",
    "train_sol = train_sol.withColumnRenamed(\"features_scaled\",\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e9271",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d90506",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a889c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_sol.randomSplit([.8,.2],seed=42)\n",
    "#lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10, regParam=0.01,elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train_data)\n",
    "predictions = lrModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35231a1f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f976d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(trainingSummary.precisionByLabel)\n",
    "print(trainingSummary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ce85e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))\n",
    "print('Test Area Under PR', evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e431d6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Analysing coefficients\n",
    "cols = train_data.columns[:lrModel.coefficientMatrix.numCols]\n",
    "for idx,val in enumerate(lrModel.coefficientMatrix.values):\n",
    "    print(cols[idx],' = ', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4ad23",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----------Cross Validation------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da6514",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226033bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01 ,0.1, 0.3]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0, .5, .8]) \\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9a71e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train_data)\n",
    "trainingSummary = cvModel.bestModel.stages[-1].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefe3db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(trainingSummary.precisionByLabel)\n",
    "print(trainingSummary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c2ed6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prediction = cvModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55a526",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_lr = cvModel.bestModel\n",
    "cols = train_data.columns[:len(best_lr.stages[-1].coefficients)]\n",
    "for idx,val in enumerate(best_lr.stages[-1].coefficients):\n",
    "    print(cols[idx],' = ', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7795f65",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_lr.stages[-1].getRegParam()\n",
    "best_lr.stages[-1].getElasticNetParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513207e8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Random Forest Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04e9ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162b7df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_sol.randomSplit([.8,.2],seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train_data)\n",
    "predictions = rfModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd4ff2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = rfModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d99ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b29f4d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))\n",
    "print('Test Area Under PR', evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd1a6e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Analysing feature importance\n",
    "cols = train_data.columns[:len(rfModel.featureImportances.values)]\n",
    "for idx,val in enumerate(rfModel.featureImportances.values):\n",
    "    print(idx, cols[idx],' = ', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2ff14",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "importances = rfModel.featureImportances.values\n",
    "feature_list = train_data.drop(\"features\",\"label\",\"labels_cat\").columns\n",
    "x_values = list(range(len(importances)))\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "plt.xticks(x_values, feature_list, rotation=40)\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e8695",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Milti-label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d93fa2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a12d02",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = spark.read.option(\"sep\", \"\\t\").csv(\"./Assignment-2/train.data\")\n",
    "sol = spark.read.option(\"sep\", \"\\t\").csv(\"./Assignment-2/train.solution\")\n",
    "sol = sol.toDF('label')\n",
    "train = train.limit(100000)\n",
    "sol = sol.limit(100000)\n",
    "cols = spark.read.csv('./Assignment-2/feature.name', sep= '\\t', header= True)\n",
    "train = train.toDF(*cols.columns)\n",
    "train = train.withColumnRenamed(\"#followers\",\"followers\") \\\n",
    "        .withColumnRenamed(\"#favorites\",\"favorites\") \\\n",
    "        .withColumnRenamed(\"#friends\",\"friends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872eb5d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201be535",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Changing data types\n",
    "train = train.drop('tweet_id')\n",
    "columns = ['followers', 'friends', 'favorites']\n",
    "train = convertColumn(train, columns, IntegerType())\n",
    "sol = convertColumn(sol, ['label'], IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e8f17",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_sol = train.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a8eb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generate a sequencial id and join. \n",
    "train_sol.createOrReplaceTempView(\"tweets\")\n",
    "train_sol = spark.sql(\"SELECT row_number() OVER (ORDER BY (SELECT NULL)) as id,* \\\n",
    "          FROM tweets\")\n",
    "sol.createOrReplaceTempView(\"sol\")\n",
    "sol = spark.sql(\"SELECT row_number() OVER (ORDER BY (SELECT NULL)) as id,* \\\n",
    "          FROM sol\")\n",
    "train_sol = train_sol.join(sol, train_sol.id == sol.id,how='left')\n",
    "train_sol = train_sol.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351c652",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Remove tweets with 0 retweets.\n",
    "train_sol = train_sol.filter(train_sol.label>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325b6b0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def categories_gen(row):\n",
    "    if row>0 and row<=4:\n",
    "        return '1-4';\n",
    "        #return 0.0\n",
    "    elif row>4 and row<=20:\n",
    "        return '5-20';\n",
    "        #return 1.0\n",
    "    elif row>20 and row<=80:\n",
    "        return '21-80';\n",
    "        return 2.0\n",
    "    else:\n",
    "        return '80+';\n",
    "        #return 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a53f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "#Converting int label to categories(string)\n",
    "#cal = F.udf(categories_gen, FloatType())\n",
    "cal = F.udf(categories_gen)\n",
    "train_sol = train_sol.withColumn(\"labels_cat\",cal(F.col(\"label\")))\n",
    "train_sol = train_sol.drop(\"label\")\n",
    "#Converting string class into index\n",
    "string_indexer = StringIndexer(inputCol=\"labels_cat\", outputCol=\"label\")\n",
    "fitted_indexer = string_indexer.fit(train_sol)\n",
    "train_sol = fitted_indexer.transform(train_sol)\n",
    "#train_sol = train_sol.withColumn(\"labels_cat\", train_sol[\"labels_cat\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ace617",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_sol.groupby(\"label\",\"labels_cat\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540c8fd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "------Undersampling----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5071022",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "row = train_sol.groupby(\"labels_cat\").count().orderBy(\"count\").take(1)\n",
    "minSamples = row[0][1]\n",
    "print(minSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4f9bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create new dataframes with smaller samples (minSamples). \n",
    "cat0Ratio=minSamples/train_sol.filter(F.col(\"labels_cat\")==0.0).count()\n",
    "cat0 = train_sol.filter(F.col(\"labels_cat\")==0.0).sample(withReplacement=False, fraction=cat0Ratio)\n",
    "\n",
    "cat1Ratio=minSamples/train_sol.filter(F.col(\"labels_cat\")==1.0).count()\n",
    "cat1 = train_sol.filter(F.col(\"labels_cat\")==1.0).sample(withReplacement=False, fraction=cat1Ratio)\n",
    "\n",
    "cat2Ratio=minSamples/train_sol.filter(F.col(\"labels_cat\")==2.0).count()\n",
    "cat2 = train_sol.filter(F.col(\"labels_cat\")==2.0).sample(withReplacement=False, fraction=cat2Ratio)\n",
    "\n",
    "cat3 = train_sol.filter(F.col(\"labels_cat\")==3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef52af",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Merge all undersampled dataframes\n",
    "train_sol = cat0.union(cat1)\n",
    "train_sol = train_sol.union(cat2)\n",
    "train_sol = train_sol.union(cat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a73b752",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_sol.groupby(\"labels_cat\").count().orderBy(\"labels_cat\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae8911",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-------Feature Engineering--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270e170",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "split_col = F.split(train_sol['sentiment'], ' ')\n",
    "train_sol = train_sol.withColumn('pos', split_col.getItem(0))\n",
    "train_sol = train_sol.withColumn('neg', split_col.getItem(1))\n",
    "train_sol = train_sol.drop(\"sentiment\")\n",
    "columns = ['pos','neg']\n",
    "train_sol = convertColumn(train_sol, columns, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21659b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Function that splits each entity and calculate the average of all the entities in one tweet\n",
    "def enti_score(row):\n",
    "    lis = []\n",
    "    for x in row:\n",
    "        arr=x.split(sep=\":\")\n",
    "        #print(arr[2])\n",
    "        try:\n",
    "            lis.append(float(arr[2]))\n",
    "        except:\n",
    "            continue\n",
    "    if len(lis) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return sum(lis) / len(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496faa4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create the UDF and create a new column with the output of the UDF\n",
    "cal = F.udf(enti_score, FloatType())\n",
    "train_sol = train_sol.withColumn(\"ent_score\",cal(F.split(\"entities\",\"[;]\")))\n",
    "train_sol = train_sol.drop(\"entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a60d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Counting mentions\n",
    "train_sol = train_sol.withColumn(\"mentions_count\",F.when(F.col(\"mentions\")!='null;',F.size(F.split(\"mentions\",\" \"))).otherwise(0))#.show()\n",
    "train_sol = train_sol.drop(\"mentions\")\n",
    "#Counting urls\n",
    "train_sol = train_sol.withColumn(\"url_count\",F.when(F.col(\"urls\")!='null;',F.size(F.split(\"urls\",\"\"\":\\-\\:\"\"\"))-1).otherwise(0))#.show()\n",
    "train_sol = train_sol.drop(\"urls\")\n",
    "#Counting hashtags\n",
    "train_sol = train_sol.withColumn(\"hash_count\",F.when(F.col(\"hashtags\")!='null;',F.size(F.split(\"hashtags\",\" \"))).otherwise(0))#.show()\n",
    "train_sol = train_sol.drop(\"hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b126157",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_sol = train_sol.drop(\"Date\",\"username\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ff100",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_sol.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ae7bf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----------Standard Scaler---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8b0a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "cols = train_sol.drop(\"labels_cat\").columns\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "inputDF = assembler.transform(train_sol)#.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648f3fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "scaler = standardScaler.fit(inputDF)\n",
    "train_sol = scaler.transform(inputDF)\n",
    "\n",
    "train_sol = train_sol.drop(\"features\")\n",
    "train_sol = train_sol.withColumnRenamed(\"features_scaled\",\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4299c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Random Forest Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7535a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07253bb8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_sol.randomSplit([.8,.2],seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labels_cat')\n",
    "rfModel = rf.fit(train_data)\n",
    "predictions = rfModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a5ffc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = rfModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99960a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3cb9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(trainingSummary.truePositiveRateByLabel)\n",
    "print(trainingSummary.recallByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97faf4e2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "?MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51077620",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "prediction = rfModel.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='labels_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c769c0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Test Area Under ROC', evaluator.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc7cf7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "------------Feature importance----------\n",
    "\n",
    "https://people.stat.sc.edu/haigang/improvement.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a299d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rfModel.featureImportances.values\n",
    "cols = train_data.drop(\"labels_cat\",\"label\").columns\n",
    "for idx,val in enumerate(rfModel.featureImportances.values):\n",
    "    print(idx, cols[idx],' = ', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3ae0d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "importances = rfModel.featureImportances.values\n",
    "feature_list = train_data.drop(\"features\",\"label\",\"labels_cat\").columns\n",
    "x_values = list(range(len(importances)))\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "plt.xticks(x_values, feature_list, rotation=40)\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4859f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "--------------Cross Validation---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7366a65",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db4af2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labels_cat')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [3 ,10 , 20])\\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800a46d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train_data)\n",
    "trainingSummary = cvModel.bestModel.stages[-1].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ba64c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(trainingSummary.precisionByLabel)\n",
    "print(trainingSummary.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a549a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_lr = cvModel.bestModel\n",
    "print(best_lr.stages[-1].getMaxBins())\n",
    "print(best_lr.stages[-1].getMaxDepth())\n",
    "best_lr.stages[-1].getNumTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b119c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
