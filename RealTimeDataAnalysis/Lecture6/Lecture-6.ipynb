{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de417b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Session-6\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a55091d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! wget 'https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/all/online-retail-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f23e8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales = spark.read.csv('online-retail-dataset.csv', header= True, inferSchema= True)\n",
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2eb6ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          265|      30|     17420|\n",
      "|          133|       8|     16861|\n",
      "|          567|      86|     16503|\n",
      "|         3065|     302|     15727|\n",
      "|         7430|     224|     17389|\n",
      "+-------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Transformer\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "basicTransformation = SQLTransformer()\\\n",
    ".setStatement(\"\"\"\n",
    "SELECT sum(Quantity), count(*), CustomerID\n",
    "FROM __THIS__\n",
    "GROUP BY CustomerID\n",
    "\"\"\")\n",
    "basicTransformation.transform(sales).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d8a9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-------+--------------+-----+\n",
      "| id|country|hour|clicked|      features|label|\n",
      "+---+-------+----+-------+--------------+-----+\n",
      "|  7|     US|  18|    1.0|[0.0,0.0,18.0]|  1.0|\n",
      "|  8|     CA|  12|    0.0|[1.0,0.0,12.0]|  0.0|\n",
      "|  9|     NZ|  15|    0.0|[0.0,1.0,15.0]|  0.0|\n",
      "+---+-------+----+-------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RFormula based feature transformation\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [(7, \"US\", 18, 1.0),\n",
    "     (8, \"CA\", 12, 0.0),\n",
    "     (9, \"NZ\", 15, 0.0)],\n",
    "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
    "\n",
    "formula = RFormula(\n",
    "    formula=\"clicked ~ country + hour\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(dataset).transform(dataset)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0b025",
   "metadata": {},
   "source": [
    "## Feature Assembling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff0c278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+----+\n",
      "|Code|Security|Month|Year|\n",
      "+----+--------+-----+----+\n",
      "| ATL|      17|    6|2003|\n",
      "| BOS|       3|    6|2003|\n",
      "| BWI|       8|    6|2003|\n",
      "| CLT|       2|    6|2003|\n",
      "| DCA|       4|    6|2003|\n",
      "+----+--------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .. means that we are going to the preceding directory beofre reading the corresponding path\n",
    "df = spark.read.json(\"../data/airlines.json\")\n",
    "airport_delays = df.select(\"Airport.Code\", \n",
    "                          \"Statistics.# of Delays.Security\", \n",
    "                          \"Time.Month\",\n",
    "                          \"Time.Year\")\n",
    "airport_delays.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3c7f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+----+------------------------------------+\n",
      "|Code|Security|Month|Year|VectorAssembler_2ed8900e8557__output|\n",
      "+----+--------+-----+----+------------------------------------+\n",
      "| ATL|      17|    6|2003|                        [6.0,2003.0]|\n",
      "| BOS|       3|    6|2003|                        [6.0,2003.0]|\n",
      "| BWI|       8|    6|2003|                        [6.0,2003.0]|\n",
      "| CLT|       2|    6|2003|                        [6.0,2003.0]|\n",
      "| DCA|       4|    6|2003|                        [6.0,2003.0]|\n",
      "+----+--------+-----+----+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector Assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"Month\", \"Year\"])\n",
    "va.transform(airport_delays).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae2577",
   "metadata": {},
   "source": [
    "## Working with Continuous features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950263a3",
   "metadata": {},
   "source": [
    "### Bucketizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "096eee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b741d",
   "metadata": {},
   "source": [
    "### Scaling and Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! wget 'https://raw.githubusercontent.com/Aditya-Mankar/Diabetes-Prediction/master/diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6a1be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"diabetes.csv\", header= True)\n",
    "df= df.select(*(col(c).cast(\"float\").alias(c) for c in df.columns))\n",
    "va = VectorAssembler().setInputCols(['Pregnancies', 'Glucose', 'BloodPressure'])\n",
    "df= va.transform(df)\n",
    "vector_column= df.columns[-1]\n",
    "df_scale= df.select(col(vector_column).alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19ae7f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------------------------------------------+\n",
      "|features        |StandardScaler_26be9854c13e__output                       |\n",
      "+----------------+----------------------------------------------------------+\n",
      "|[6.0,148.0,72.0]|[1.7806383732194306,4.628960915766174,3.7198138711154307] |\n",
      "|[1.0,85.0,66.0] |[0.29677306220323846,2.658524850271114,3.4098293818558116]|\n",
      "|[8.0,183.0,64.0]|[2.3741844976259077,5.723647618818986,3.306501218769272]  |\n",
      "|[1.0,89.0,66.0] |[0.29677306220323846,2.783631902048578,3.4098293818558116]|\n",
      "|[0.0,137.0,40.0]|[0.0,4.284916523378148,2.0665632617307947]                |\n",
      "+----------------+----------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard Scaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(df_scale).transform(df_scale).show(5, truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa21e7",
   "metadata": {},
   "source": [
    "## Working with Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "34483a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip'\n",
    "! unzip 'bank.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e3174018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+---+\n",
      "|        job|marital|education|  y|\n",
      "+-----------+-------+---------+---+\n",
      "| unemployed|married|  primary| no|\n",
      "|   services|married|secondary| no|\n",
      "| management| single| tertiary| no|\n",
      "| management|married| tertiary| no|\n",
      "|blue-collar|married|secondary| no|\n",
      "+-----------+-------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank= spark.read.csv(\"bank.csv\", header= True, inferSchema= True, sep= ';')\n",
    "bank.select(\"job\", \"marital\", \"education\", \"y\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4862ae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|          job|job_x|\n",
      "+-------------+-----+\n",
      "|   unemployed|  8.0|\n",
      "|     services|  4.0|\n",
      "|   management|  0.0|\n",
      "|   management|  0.0|\n",
      "|  blue-collar|  1.0|\n",
      "|   management|  0.0|\n",
      "|self-employed|  6.0|\n",
      "|   technician|  2.0|\n",
      "| entrepreneur|  7.0|\n",
      "|     services|  4.0|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "sringIndexer = StringIndexer().setInputCol(\"job\").setOutputCol(\"job_x\")\n",
    "bank= sringIndexer.fit(bank).transform(bank)\n",
    "bank.select(\"job\", \"job_x\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7142a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+--------------+\n",
      "|job          |job_x|job_ohe       |\n",
      "+-------------+-----+--------------+\n",
      "|unemployed   |8.0  |(11,[8],[1.0])|\n",
      "|services     |4.0  |(11,[4],[1.0])|\n",
      "|management   |0.0  |(11,[0],[1.0])|\n",
      "|management   |0.0  |(11,[0],[1.0])|\n",
      "|blue-collar  |1.0  |(11,[1],[1.0])|\n",
      "|management   |0.0  |(11,[0],[1.0])|\n",
      "|self-employed|6.0  |(11,[6],[1.0])|\n",
      "|technician   |2.0  |(11,[2],[1.0])|\n",
      "|entrepreneur |7.0  |(11,[7],[1.0])|\n",
      "|services     |4.0  |(11,[4],[1.0])|\n",
      "+-------------+-----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCols=[\"job_x\"],\n",
    "                        outputCols=[\"job_ohe\"])\n",
    "model = encoder.fit(bank)\n",
    "encoded = model.transform(bank)\n",
    "encoded.select(\"job\", \"job_x\", \"job_ohe\").show(10, truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ce9f7",
   "metadata": {},
   "source": [
    "## Working with Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "47b156f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |Tokenized_Description                     |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[white, hanging, heart, t-light, holder]  |\n",
      "|WHITE METAL LANTERN                |[white, metal, lantern]                   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[cream, cupid, hearts, coat, hanger]      |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[knitted, union, flag, hot, water, bottle]|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[red, woolly, hottie, white, heart.]      |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sales data\n",
    "# Tokenization using WhiteSpace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"Tokenized_Description\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a87f6028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------------------------+\n",
      "|Description                        |Tokenized_Description                |\n",
      "+-----------------------------------+-------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[white hanging heart t, light holder]|\n",
      "|WHITE METAL LANTERN                |[white metal lantern]                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[cream cupid hearts coat hanger]     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[knitted union flag hot water bottle]|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[red woolly hottie white heart.]     |\n",
      "+-----------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using Hyphen \n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    ".setInputCol(\"Description\")\\\n",
    ".setOutputCol(\"Tokenized_Description\")\\\n",
    ".setPattern(\"-\")\\\n",
    ".setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f3cd24b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------------------------------+\n",
      "|         Description|Tokenized_Description|StopWordsRemover_b1e9d2b661bf__output|\n",
      "+--------------------+---------------------+-------------------------------------+\n",
      "|WHITE HANGING HEA...| [white, hanging, ...|                 [white, hanging, ...|\n",
      "| WHITE METAL LANTERN| [white, metal, la...|                 [white, metal, la...|\n",
      "|CREAM CUPID HEART...| [cream, cupid, he...|                 [cream, cupid, he...|\n",
      "|KNITTED UNION FLA...| [knitted, union, ...|                 [knitted, union, ...|\n",
      "|RED WOOLLY HOTTIE...| [red, woolly, hot...|                 [red, woolly, hot...|\n",
      "|SET 7 BABUSHKA NE...| [set, 7, babushka...|                 [set, 7, babushka...|\n",
      "|GLASS STAR FROSTE...| [glass, star, fro...|                 [glass, star, fro...|\n",
      "|HAND WARMER UNION...| [hand, warmer, un...|                 [hand, warmer, un...|\n",
      "|HAND WARMER RED P...| [hand, warmer, re...|                 [hand, warmer, re...|\n",
      "|ASSORTED COLOUR B...| [assorted, colour...|                 [assorted, colour...|\n",
      "|POPPY'S PLAYHOUSE...| [poppy's, playhou...|                 [poppy's, playhou...|\n",
      "|POPPY'S PLAYHOUSE...| [poppy's, playhou...|                 [poppy's, playhou...|\n",
      "|FELTCRAFT PRINCES...| [feltcraft, princ...|                 [feltcraft, princ...|\n",
      "|IVORY KNITTED MUG...| [ivory, knitted, ...|                 [ivory, knitted, ...|\n",
      "|BOX OF 6 ASSORTED...| [box, of, 6, asso...|                 [box, 6, assorted...|\n",
      "|BOX OF VINTAGE JI...| [box, of, vintage...|                 [box, vintage, ji...|\n",
      "|BOX OF VINTAGE AL...| [box, of, vintage...|                 [box, vintage, al...|\n",
      "|HOME BUILDING BLO...| [home, building, ...|                 [home, building, ...|\n",
      "|LOVE BUILDING BLO...| [love, building, ...|                 [love, building, ...|\n",
      "|RECIPE BOX WITH M...| [recipe, box, wit...|                 [recipe, box, met...|\n",
      "+--------------------+---------------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    ".setStopWords(englishStopWords)\\\n",
    ".setInputCol(\"Tokenized_Description\")\n",
    "stops.transform(tokenized).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word combinations- NGrams\n",
    "from pyspark.ml.feature import NGram\n",
    "bigram = NGram().setInputCol(\"Tokenized_Description\").setN(2)\n",
    "bigram.transform(tokenized.select(\"Tokenized_Description\")).show(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f5aeaee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------------------------------+\n",
      "|Description                    |Tokenized_Description               |\n",
      "+-------------------------------+------------------------------------+\n",
      "|RED WOOLLY HOTTIE WHITE HEART. |[red, woolly, hottie, white, heart.]|\n",
      "|HAND WARMER RED POLKA DOT      |[hand, warmer, red, polka, dot]     |\n",
      "|RED COAT RACK PARIS FASHION    |[red, coat, rack, paris, fashion]   |\n",
      "|ALARM CLOCK BAKELIKE RED       |[alarm, clock, bakelike, red]       |\n",
      "|SET/2 RED RETROSPOT TEA TOWELS |[set/2, red, retrospot, tea, towels]|\n",
      "|RED TOADSTOOL LED NIGHT LIGHT  |[red, toadstool, led, night, light] |\n",
      "|HAND WARMER RED POLKA DOT      |[hand, warmer, red, polka, dot]     |\n",
      "|EDWARDIAN PARASOL RED          |[edwardian, parasol, red]           |\n",
      "|RED WOOLLY HOTTIE WHITE HEART. |[red, woolly, hottie, white, heart.]|\n",
      "|EDWARDIAN PARASOL RED          |[edwardian, parasol, red]           |\n",
      "+-------------------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfIdfIn = tokenized\\\n",
    ".where(\"array_contains(Tokenized_Description, 'red')\")\\\n",
    ".limit(10)\n",
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1ae813ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------------------+\n",
      "|Tokenized_Description|               TFOut|              IDFOut|\n",
      "+---------------------+--------------------+--------------------+\n",
      "| [red, woolly, hot...|(10000,[52,388,69...|(10000,[52,388,69...|\n",
      "| [hand, warmer, re...|(10000,[52,3197,3...|(10000,[52,3197,3...|\n",
      "| [red, coat, rack,...|(10000,[52,477,20...|(10000,[52,477,20...|\n",
      "| [alarm, clock, ba...|(10000,[52,4995,8...|(10000,[52,4995,8...|\n",
      "| [set/2, red, retr...|(10000,[52,129,29...|(10000,[52,129,29...|\n",
      "| [red, toadstool, ...|(10000,[52,773,17...|(10000,[52,773,17...|\n",
      "| [hand, warmer, re...|(10000,[52,3197,3...|(10000,[52,3197,3...|\n",
      "| [edwardian, paras...|(10000,[52,2397,5...|(10000,[52,2397,5...|\n",
      "| [red, woolly, hot...|(10000,[52,388,69...|(10000,[52,388,69...|\n",
      "| [edwardian, paras...|(10000,[52,2397,5...|(10000,[52,2397,5...|\n",
      "+---------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "tf = HashingTF()\\\n",
    ".setInputCol(\"Tokenized_Description\")\\\n",
    ".setOutputCol(\"TFOut\")\\\n",
    ".setNumFeatures(10000)\n",
    "\n",
    "idf = IDF()\\\n",
    ".setInputCol(\"TFOut\")\\\n",
    ".setOutputCol(\"IDFOut\")\\\n",
    ".setMinDocFreq(2)\n",
    "\n",
    "idf.fit(tf.transform(tfIdfIn))\\\n",
    ".transform(tf.transform(tfIdfIn))\\\n",
    ".select(\"Tokenized_Description\", \"TFOut\", \"IDFOut\")\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6c3df",
   "metadata": {},
   "source": [
    "## Feature Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d32eed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "df = spark.read.csv(\"diabetes.csv\", header= True)\n",
    "df= df.select(*(col(c).cast(\"float\").alias(c) for c in df.columns))\n",
    "\n",
    "# Vector Assembler to assemble all the features into a single vector\n",
    "va = VectorAssembler().setInputCols(['Pregnancies', 'Glucose', 'BloodPressure'])\n",
    "df= va.transform(df)\n",
    "vector_column= df.columns[-1]\n",
    "df_scale= df.select(col(vector_column).alias(\"features\"))\n",
    "\n",
    "# Standard Scaler to scale the data\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "df_scale= sScaler.fit(df_scale).transform(df_scale)\n",
    "scaled_column= df.columns[-1]\n",
    "df_scale= df.select(col(scaled_column).alias(\"scaled_features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a417eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------------------------+\n",
      "|scaled_features |principal_components                    |\n",
      "+----------------+----------------------------------------+\n",
      "|[6.0,148.0,72.0]|[-156.77339030024655,50.38873651658909] |\n",
      "|[1.0,85.0,66.0] |[-93.49160365513833,53.28385715428625]  |\n",
      "|[8.0,183.0,64.0]|[-190.31428823265458,37.548082093334486]|\n",
      "|[1.0,89.0,66.0] |[-97.45089137263243,52.716540398185955] |\n",
      "|[0.0,137.0,40.0]|[-141.2674182991563,20.15663231791124]  |\n",
      "+----------------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA().setInputCol(\"scaled_features\").setK(2)\n",
    "pca= pca.fit(df_scale).transform(df_scale)\n",
    "pca_features= pca.columns[-1]\n",
    "pca.select(col(\"scaled_features\"), col(pca_features).alias(\"principal_components\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2b3c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------------------+\n",
      "|scaled_features |PolynomialExpansion_f4e60579d79f__output                 |\n",
      "+----------------+---------------------------------------------------------+\n",
      "|[6.0,148.0,72.0]|[6.0,36.0,148.0,888.0,21904.0,72.0,432.0,10656.0,5184.0] |\n",
      "|[1.0,85.0,66.0] |[1.0,1.0,85.0,85.0,7225.0,66.0,66.0,5610.0,4356.0]       |\n",
      "|[8.0,183.0,64.0]|[8.0,64.0,183.0,1464.0,33489.0,64.0,512.0,11712.0,4096.0]|\n",
      "|[1.0,89.0,66.0] |[1.0,1.0,89.0,89.0,7921.0,66.0,66.0,5874.0,4356.0]       |\n",
      "|[0.0,137.0,40.0]|[0.0,0.0,137.0,0.0,18769.0,40.0,0.0,5480.0,1600.0]       |\n",
      "+----------------+---------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "pe = PolynomialExpansion().setInputCol(\"scaled_features\").setDegree(2)\n",
    "pe.transform(df_scale).show(5, truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00313e5",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "191176f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 1 features selected\n",
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chi-square based feature selection\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" \n",
    "      % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd7e98",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e04247ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"Tokenized_Description\")\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover().setStopWords(englishStopWords)\\\n",
    ".setInputCol(\"Tokenized_Description\")\\\n",
    ".setOutputCol(\"Removed_Stopwords\")\n",
    "\n",
    "tf = HashingTF()\\\n",
    ".setInputCol(\"Removed_Stopwords\")\\\n",
    ".setOutputCol(\"TFOut\")\\\n",
    ".setNumFeatures(10000)\n",
    "\n",
    "stages= [tkn, stops, tf]\n",
    "pipeline = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9064a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------------------+\n",
      "|Tokenized_Description|   Removed_Stopwords|               TFOut|\n",
      "+---------------------+--------------------+--------------------+\n",
      "| [white, hanging, ...|[white, hanging, ...|(10000,[4618,4667...|\n",
      "| [white, metal, la...|[white, metal, la...|(10000,[426,671,5...|\n",
      "| [cream, cupid, he...|[cream, cupid, he...|(10000,[477,6575,...|\n",
      "| [knitted, union, ...|[knitted, union, ...|(10000,[2352,2589...|\n",
      "| [red, woolly, hot...|[red, woolly, hot...|(10000,[52,388,69...|\n",
      "+---------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(sales)\n",
    "df = pipelineModel.transform(sales)\n",
    "df = df.select(\"Tokenized_Description\", \"Removed_Stopwords\", \"TFOut\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc7ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
