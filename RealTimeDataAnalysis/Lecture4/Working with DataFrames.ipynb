{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First contact with df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be loaded in through a CSV, JSON, XML, or a Parquet file. \n",
    "\n",
    "It can also be created using an existing RDD and through any other database, like Hive or Cassandra as well. \n",
    "\n",
    "It can also take in data from HDFS or the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import time\n",
    "import operator\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\")\n",
    "conf.setAppName(\"spark-basic\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first example with 'fake' data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6 },\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William', 5: 'Jack'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male', 5: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0, 5: 0}}\n",
    "\n",
    "data2 = {'Id': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Owen</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Florence</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Lily</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>William</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId      Name     Sex  Survived\n",
       "0            1      Owen    male         0\n",
       "1            2  Florence  female         1\n",
       "2            3     Laina  female         1\n",
       "3            4      Lily  female         1\n",
       "4            5   William    male         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a spark session to process this type of data. Caution, many examples on the Internet do not specify what \"spark.\" is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          6|    Jack|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=1, Name='Owen', Sex='male', Survived=0),\n",
       " Row(PassengerId=2, Name='Florence', Sex='female', Survived=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the properties of our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating with dataframes\n",
    "\n",
    "If you have experience coding with the package dplyr from R, you have more than half of it covered!\n",
    "\n",
    "**Filter**, **select**, **mutate**, **summarize** and **arrange** are the basic verbs here too, although some of them have different names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    Name|   Sex|\n",
      "+--------+------+\n",
      "|    Owen|  male|\n",
      "|Florence|female|\n",
      "|   Laina|female|\n",
      "|    Lily|female|\n",
      "| William|  male|\n",
      "|    Jack|  male|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select([\"Name\",\"Sex\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.Sex == 'female').filter(df1.Survived == 1).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also do it SQL-style. Note that this is not a python syntax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(\"Sex='female'\").show()#We can run sql inside! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the equivalent of mutate...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hello = df2.withColumn('PricePerYear', df2.Fare/df2.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: bigint, Age: bigint, Fare: double, Pclass: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: bigint, Age: bigint, Fare: double, Pclass: bigint, PricePerYear: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby is needed when we want to summarize our data... the use is identical than in lists or dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').avg(*[\"Age\",\"Fare\"]).show()\n",
    "#* is used to say use a list of columns. Not common in python but since spark is build on top of scala, then this happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').agg({'Age': 'avg', 'Fare': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with the syntax *****, it refers to \"unpack\", so we have to provide an unpacked list.\n",
    "\n",
    "If we want multiple functions at the same time, we need an dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     1|       2|              36.5|    124.4|\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').agg({'*': 'count', 'Age': 'avg', 'Fare': 'sum'}).show()\n",
    "#This star means that you want to aggregate everything, count in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present the summarized data in a nicer format, we can un toDF() to rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     1|     2|              36.5|     124.4|\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  (\n",
    "    df2.groupby('Pclass').agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\n",
    "    .show()\n",
    "  )\n",
    "#equivalent to using as '' on sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can **arrange** dataframes using the method sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+------+\n",
      "| Id|Age|Fare|Pclass|\n",
      "+---+---+----+------+\n",
      "|  1| 22| 7.3|     3|\n",
      "|  3| 26| 7.9|     3|\n",
      "|  4| 35|53.1|     1|\n",
      "|  5| 35| 8.0|     3|\n",
      "|  2| 38|71.3|     1|\n",
      "+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Age', ascending=True).show()\n",
    "#sort by in sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good reference: https://dzone.com/articles/pyspark-dataframe-tutorial-introduction-to-datafra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic joins and unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived| Id|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|          5| William|  male|       0|  5| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0|  1| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1|  3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|  2| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1|  4| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df2.Id == df1.PassengerId).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          6|    Jack|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it symmetric? Where is Jack? :'(\n",
    "\n",
    "If you have some time, explore with other types of join, such as \"left\", \"right\", \"cross\", \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+----+----+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|  Id| Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+----+----+----+------+\n",
      "|          6|    Jack|  male|       0|null|null|null|  null|\n",
      "|          5| William|  male|       0|   5|  35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0|   1|  22| 7.3|     3|\n",
      "|          3|   Laina|female|       1|   3|  26| 7.9|     3|\n",
      "|          2|Florence|female|       1|   2|  38|71.3|     1|\n",
      "|          4|    Lily|female|       1|   4|  35|53.1|     1|\n",
      "+-----------+--------+------+--------+----+----+----+------+\n",
      "\n",
      "+----+----+----+------+-----------+--------+------+--------+\n",
      "|  Id| Age|Fare|Pclass|PassengerId|    Name|   Sex|Survived|\n",
      "+----+----+----+------+-----------+--------+------+--------+\n",
      "|null|null|null|  null|          6|    Jack|  male|       0|\n",
      "|   5|  35| 8.0|     3|          5| William|  male|       0|\n",
      "|   1|  22| 7.3|     3|          1|    Owen|  male|       0|\n",
      "|   3|  26| 7.9|     3|          3|   Laina|female|       1|\n",
      "|   2|  38|71.3|     1|          2|Florence|female|       1|\n",
      "|   4|  35|53.1|     1|          4|    Lily|female|       1|\n",
      "+----+----+----+------+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Your code here\n",
    "df1.join(df2, df2.Id == df1.PassengerId, how='left').show()\n",
    "df2.join(df1, df2.Id == df1.PassengerId, how='right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+----+----+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|  Id| Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+----+----+----+------+\n",
      "|          6|    Jack|  male|       0|null|null|null|  null|\n",
      "|          5| William|  male|       0|   5|  35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0|   1|  22| 7.3|     3|\n",
      "|          3|   Laina|female|       1|   3|  26| 7.9|     3|\n",
      "|          2|Florence|female|       1|   2|  38|71.3|     1|\n",
      "|          4|    Lily|female|       1|   4|  35|53.1|     1|\n",
      "+-----------+--------+------+--------+----+----+----+------+\n",
      "\n",
      "+----+----+----+------+-----------+--------+------+--------+\n",
      "|  Id| Age|Fare|Pclass|PassengerId|    Name|   Sex|Survived|\n",
      "+----+----+----+------+-----------+--------+------+--------+\n",
      "|null|null|null|  null|          6|    Jack|  male|       0|\n",
      "|   5|  35| 8.0|     3|          5| William|  male|       0|\n",
      "|   1|  22| 7.3|     3|          1|    Owen|  male|       0|\n",
      "|   3|  26| 7.9|     3|          3|   Laina|female|       1|\n",
      "|   2|  38|71.3|     1|          2|Florence|female|       1|\n",
      "|   4|  35|53.1|     1|          4|    Lily|female|       1|\n",
      "+----+----+----+------+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df2.Id == df1.PassengerId, how='outer').show()\n",
    "df2.join(df1, df2.Id == df1.PassengerId, how='outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+------+-----------+--------+------+--------+\n",
      "| Id|Age|Fare|Pclass|PassengerId|    Name|   Sex|Survived|\n",
      "+---+---+----+------+-----------+--------+------+--------+\n",
      "|  1| 22| 7.3|     3|          1|    Owen|  male|       0|\n",
      "|  1| 22| 7.3|     3|          2|Florence|female|       1|\n",
      "|  1| 22| 7.3|     3|          3|   Laina|female|       1|\n",
      "|  1| 22| 7.3|     3|          4|    Lily|female|       1|\n",
      "|  1| 22| 7.3|     3|          5| William|  male|       0|\n",
      "|  1| 22| 7.3|     3|          6|    Jack|  male|       0|\n",
      "|  2| 38|71.3|     1|          1|    Owen|  male|       0|\n",
      "|  2| 38|71.3|     1|          2|Florence|female|       1|\n",
      "|  2| 38|71.3|     1|          3|   Laina|female|       1|\n",
      "|  2| 38|71.3|     1|          4|    Lily|female|       1|\n",
      "|  2| 38|71.3|     1|          5| William|  male|       0|\n",
      "|  2| 38|71.3|     1|          6|    Jack|  male|       0|\n",
      "|  3| 26| 7.9|     3|          1|    Owen|  male|       0|\n",
      "|  3| 26| 7.9|     3|          2|Florence|female|       1|\n",
      "|  3| 26| 7.9|     3|          3|   Laina|female|       1|\n",
      "|  3| 26| 7.9|     3|          4|    Lily|female|       1|\n",
      "|  3| 26| 7.9|     3|          5| William|  male|       0|\n",
      "|  3| 26| 7.9|     3|          6|    Jack|  male|       0|\n",
      "|  4| 35|53.1|     1|          1|    Owen|  male|       0|\n",
      "|  4| 35|53.1|     1|          2|Florence|female|       1|\n",
      "+---+---+----+------+-----------+--------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.join(df1, how='cross').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And union doesn't do what we could expect... why would we need it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          6|    Jack|  male|       0|\n",
      "|          1|      22|   7.3|       3|\n",
      "|          2|      38|  71.3|       1|\n",
      "|          3|      26|   7.9|       3|\n",
      "|          4|      35|  53.1|       1|\n",
      "|          5|      35|   8.0|       3|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()#DOING THIS DOESN MAKE SENSE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some weird join procedure... what is going on here? Are you able to find why would it be useful? (and dangerous?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived| Id|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|          1|    Owen|  male|       0|  1| 22| 7.3|     3|\n",
      "|          1|    Owen|  male|       0|  2| 38|71.3|     1|\n",
      "|          1|    Owen|  male|       0|  3| 26| 7.9|     3|\n",
      "|          1|    Owen|  male|       0|  4| 35|53.1|     1|\n",
      "|          1|    Owen|  male|       0|  5| 35| 8.0|     3|\n",
      "|          2|Florence|female|       1|  2| 38|71.3|     1|\n",
      "|          2|Florence|female|       1|  3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|  4| 35|53.1|     1|\n",
      "|          2|Florence|female|       1|  5| 35| 8.0|     3|\n",
      "|          3|   Laina|female|       1|  3| 26| 7.9|     3|\n",
      "|          3|   Laina|female|       1|  4| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1|  5| 35| 8.0|     3|\n",
      "|          4|    Lily|female|       1|  4| 35|53.1|     1|\n",
      "|          4|    Lily|female|       1|  5| 35| 8.0|     3|\n",
      "|          5| William|  male|       0|  5| 35| 8.0|     3|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.PassengerId <= df2.Id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... what about doing joins with SQL?\n",
    "\n",
    "If we want to do that, we will need temporary Views! Sounds worse than it actually is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived| Id|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|          5| William|  male|       0|  5| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0|  1| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1|  3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|  2| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1|  4| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')\n",
    "#Create temporary views first, \n",
    "\n",
    "query = '''\n",
    "    select *\n",
    "    from df1_temp a, df2_temp b\n",
    "    where a.PassengerId = b.Id'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+---------------+\n",
      "|Clase|             Media|round(Media, 2)|\n",
      "+-----+------------------+---------------+\n",
      "|    1|              36.5|           36.5|\n",
      "|    3|27.666666666666668|          27.67|\n",
      "+-----+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "resumen = df2.groupby('Pclass').avg(\"Age\").toDF(\"Clase\",\"Media\")\n",
    "resumen.select(\"*\",round(col(\"Media\"),2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and exporting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\n",
      "Building wheels for collected packages: et-xmlfile\n",
      "  Building wheel for et-xmlfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for et-xmlfile: filename=et_xmlfile-1.0.1-py3-none-any.whl size=8913 sha256=6a9211e9c2cb0c1d40d8adcdc399085c7274220da90ec45927a8401328b37a88\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/6e/df/38/abda47b884e3e25f9f9b6430e5ce44c47670758a50c0c51759\n",
      "Successfully built et-xmlfile\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.0.1 openpyxl-3.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indirectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = pd.read_excel(\"FoodMarket.xlsx\",sheet_name=\"Products\")\n",
    "cust = pd.read_excel(\"FoodMarket.xlsx\",sheet_name=\"Customers\")\n",
    "purc = pd.read_excel(\"FoodMarket.xlsx\",sheet_name=\"Purchases\")\n",
    "sell = pd.read_excel(\"FoodMarket.xlsx\",sheet_name=\"Sellers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prod = spark.createDataFrame(prod)\n",
    "cust = spark.createDataFrame(cust)\n",
    "purc = spark.createDataFrame(purc)\n",
    "sell = spark.createDataFrame(sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+--------+------+\n",
      "|Code|Buyer|Product|Quantity|Seller|\n",
      "+----+-----+-------+--------+------+\n",
      "|   1|  139|     14|       7|    33|\n",
      "|   2|  127|     51|       3|    17|\n",
      "|   3|  137|     10|      15|    22|\n",
      "|   4|   58|     54|       3|    30|\n",
      "|   5|  196|     57|       8|    11|\n",
      "|   6|  135|     19|       6|     3|\n",
      "|   7|   28|     59|      19|    10|\n",
      "|   8|  193|     60|      18|    18|\n",
      "|   9|   68|      5|       3|    33|\n",
      "|  10|   30|     51|      12|     3|\n",
      "|  11|   80|      8|       9|    29|\n",
      "|  12|   90|     12|       3|    12|\n",
      "|  13|   72|     30|       7|    38|\n",
      "|  14|  174|     53|      11|    15|\n",
      "|  15|  167|     35|      11|    45|\n",
      "|  16|  125|     13|      16|    21|\n",
      "|  17|  120|     24|       1|    20|\n",
      "|  18|  141|     28|      16|    13|\n",
      "|  19|  198|     35|      11|    49|\n",
      "|  20|   42|     32|       6|    52|\n",
      "+----+-----+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prod.show()\n",
    "purc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/jovyan/work/FoodMarket.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-17a15010a6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#RIGHT WAY OF LOADING.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#spark.read.csv(\"C:/Users/joangj/OneDrive - NETMIND/BTS/2020/data/FoodMarket.csv\",sep=\";\",header = True).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FoodMarket.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/jovyan/work/FoodMarket.csv"
     ]
    }
   ],
   "source": [
    "#RIGHT WAY OF LOADING. \n",
    "#spark.read.csv(\"C:/Users/joangj/OneDrive - NETMIND/BTS/2020/data/FoodMarket.csv\",sep=\";\",header = True).show()\n",
    "spark.read.csv(\"FoodMarket.csv\",sep=\";\",header = True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you solve the problem of the decimal format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing the file back again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way, but generates a new folder and extra files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/joangj/OneDrive - NETMIND/BTS/2020/RealTime/Newdata already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-33ef55dc9da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Newdata\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/joangj/OneDrive - NETMIND/BTS/2020/RealTime/Newdata already exists.;"
     ]
    }
   ],
   "source": [
    "prod.write.csv(\"Newdata\",header = True) \n",
    "#Send back the data to a cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.toPandas().to_csv('mycsv.csv')\n",
    "#Do it with small files, not big ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many partitions do we have in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purc.rdd.getNumPartitions() \n",
    "#In how any partitions the data is splited.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = purc.groupBy(\"Seller\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Seller|count|\n",
      "+------+-----+\n",
      "|    29|   32|\n",
      "|    26|   25|\n",
      "|    19|   28|\n",
      "|    54|   29|\n",
      "|    22|   37|\n",
      "|     7|   34|\n",
      "|    34|   23|\n",
      "|    50|   29|\n",
      "|    57|   18|\n",
      "|    32|   18|\n",
      "|    43|   28|\n",
      "|    31|   26|\n",
      "|    39|   39|\n",
      "|    25|   33|\n",
      "|     6|   17|\n",
      "|    58|   20|\n",
      "|     9|   21|\n",
      "|    27|   20|\n",
      "|    56|   25|\n",
      "|    51|   18|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.rdd.getNumPartitions() \n",
    "#After working with the data, then the number of partitions is allways 200 by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the partition number suddenly increases. This is due to the fact that the Spark SQL module contains the following default configuration: spark.sql.shuffle.partitions set to 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your use case, this can be benefitial or harmfull. In this particular case, one can see that the data volume is not enough to fill all the partitions when there are 200 of them, which causes unnecessary map reduce operations, and ultimately causes the creation of very small files in HDFS, which is not desirable.\n",
    "\n",
    "If we want to change this parameter (and many others) we can do it through the SQLContext..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"10\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
