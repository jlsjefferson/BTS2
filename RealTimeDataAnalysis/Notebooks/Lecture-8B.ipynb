{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2119942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b76db45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('Elephas_App').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5969c26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Elephas_App>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1709be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]')\\\n",
    ".appName('deep-learning').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d7627f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "def shuffle_csv(csv_file):\n",
    "    lines = open(csv_file).readlines()\n",
    "    random.shuffle(lines)\n",
    "    open(csv_file, 'w').writelines(lines)\n",
    "\n",
    "def load_data_frame(csv_file, shuffle=True, train=True):\n",
    "    if shuffle:\n",
    "        shuffle_csv(csv_file)\n",
    "    data = sc.textFile(data_path + csv_file)\n",
    "    # This is an RDD, which will later be transformed to a data frame\n",
    "    data = data.filter(lambda x:x.split(',')[0] != 'id').map(lambda line: line.split(','))\n",
    "    if train:\n",
    "        data = data.map(\n",
    "            lambda line: (Vectors.dense(np.asarray(line[1:-1]).astype(np.float32)),\n",
    "                          str(line[-1])) )\n",
    "    else:\n",
    "        # Test data gets dummy labels. We need the same structure as in Train data\n",
    "        data = data.map( lambda line: (Vectors.dense(np.asarray(line[1:]).astype(np.float32)),\"Class_1\") ) \n",
    "    return sql_context.createDataFrame(data, ['features', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20dc6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data frame:\n",
      "+--------------------+--------+\n",
      "|            features|category|\n",
      "+--------------------+--------+\n",
      "|[0.0,0.0,1.0,0.0,...| Class_3|\n",
      "|[1.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|\n",
      "|[0.0,0.0,8.0,6.0,...| Class_6|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[1.0,0.0,0.0,0.0,...| Class_8|\n",
      "|[0.0,1.0,6.0,1.0,...| Class_6|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test data frame (note the dummy category):\n",
      "+--------------------+--------+\n",
      "|            features|category|\n",
      "+--------------------+--------+\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[2.0,2.0,14.0,16....| Class_1|\n",
      "|[0.0,1.0,12.0,1.0...| Class_1|\n",
      "|[0.0,0.0,0.0,1.0,...| Class_1|\n",
      "|[1.0,0.0,0.0,1.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[2.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data_frame(\"../data_path/train.csv\")\n",
    "test_df = load_data_frame(\"../data_path/test.csv\", shuffle=False, train=False) # No need to shuffle test data\n",
    "\n",
    "print(\"Train data frame:\")\n",
    "train_df.show(10)\n",
    "\n",
    "print(\"Test data frame (note the dummy category):\")\n",
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3bdaa4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "fitted_indexer = string_indexer.fit(train_df)\n",
    "indexed_df = fitted_indexer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46091bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "fitted_scaler = scaler.fit(indexed_df)\n",
    "scaled_df = fitted_scaler.transform(indexed_df)\n",
    "print(\"The result of indexing and scaling. Each transformation adds new columns to the data frame:\")\n",
    "scaled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e802993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of indexing and scaling. Each transformation adds new columns to the data frame:\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|            features|category|label|     scaled_features|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|[0.0,0.0,1.0,0.0,...| Class_3|  3.0|[-0.2535060296260...|\n",
      "|[1.0,0.0,0.0,0.0,...| Class_2|  0.0|[0.40208999583479...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|  0.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|  2.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,8.0,6.0,...| Class_6|  1.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|  2.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|  0.0|[-0.2535060296260...|\n",
      "|[1.0,0.0,0.0,0.0,...| Class_8|  2.0|[0.40208999583479...|\n",
      "|[0.0,1.0,6.0,1.0,...| Class_6|  1.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|  0.0|[-0.2535060296260...|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2548ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "nb_classes = train_df.select(\"category\").distinct().count()\n",
    "input_dim = len(train_df.select(\"features\").first()[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(input_dim,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a2516d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_ff673344560a"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elephas.ml_model import ElephasEstimator\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephasEstimator()\n",
    "# The next two paramters come directly from pyspark\n",
    "estimator.setFeaturesCol(\"scaled_features\")             \n",
    "estimator.setLabelCol(\"label\")                \n",
    "# Provide serialized Keras model to the estimator object\n",
    "estimator.set_keras_model_config(model.to_yaml())\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(5) \n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"categorical_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "409d36f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb1fe83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaModelWrapper.__del__ at 0x161847290>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pyspark/mllib/common.py\", line 137, in __del__\n",
      "    self._sc._gateway.detach(self._java_model)\n",
      "AttributeError: 'MulticlassMetrics' object has no attribute '_sc'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Synchronous training complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, scaler, estimator])\n",
    "fitted_pipeline = pipeline.fit(train_df) \n",
    "prediction = fitted_pipeline.transform(train_df)\n",
    "# prediction = fitted_pipeline.transform(test_df) # <-- The same code evaluates test data.\n",
    "pnl = prediction.select(\"label\", \"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cafdd3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|          prediction|\n",
      "+-----+--------------------+\n",
      "|  3.0|[0.50407135486602...|\n",
      "|  0.0|[0.52967709302902...|\n",
      "|  0.0|[0.50407135486602...|\n",
      "|  2.0|[0.00933281425386...|\n",
      "|  1.0|[0.0, 1.0, 0.0, 0...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pnl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "92ca9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "metrics_df= prediction.toPandas()\n",
    "labels= metrics_df[\"label\"].values\n",
    "predictions= [np.argmax(val) for val in np.array(metrics_df[\"prediction\"].values)]\n",
    "accuracy_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccc77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
