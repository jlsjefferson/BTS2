{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "import re,string\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:07<00:00, 1754.21it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_pos = 'AclImdb/test/pos'\n",
    "arr = []\n",
    "for filename in tqdm(os.listdir(directory_pos)):\n",
    "    path= os.path.join(directory_pos,filename)\n",
    "    with open(path) as f:\n",
    "        review = f.readlines()\n",
    "        arr.append(review)\n",
    "        \n",
    "posArr = np.array(arr)\n",
    "posArr = np.insert(posArr, 1, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:06<00:00, 1860.13it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_neg = 'AclImdb/test/neg'\n",
    "arr = []\n",
    "for filename in tqdm(os.listdir(directory_neg)):\n",
    "    path= os.path.join(directory_neg,filename)\n",
    "    with open(path) as f:\n",
    "        review = f.readlines()\n",
    "        arr.append(review)\n",
    "\n",
    "negArr = np.array(arr)\n",
    "negArr = np.insert(negArr, 1, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:03<00:00, 3232.90it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_pos_1 = 'AclImdb/train/pos'\n",
    "arr = []\n",
    "for filename in tqdm(os.listdir(directory_pos_1)):\n",
    "    path= os.path.join(directory_pos_1,filename)\n",
    "    with open(path) as f:\n",
    "        review = f.readlines()\n",
    "        arr.append(review)\n",
    "        \n",
    "posArr1 = np.array(arr)\n",
    "posArr1 = np.insert(posArr1, 1, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:06<00:00, 1845.35it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_neg_1 = 'AclImdb/train/neg'\n",
    "arr = []\n",
    "for filename in tqdm(os.listdir(directory_neg_1)):\n",
    "    path= os.path.join(directory_neg_1,filename)\n",
    "    with open(path) as f:\n",
    "        review = f.readlines()\n",
    "        arr.append(review)\n",
    "\n",
    "negArr1 = np.array(arr)\n",
    "negArr1 = np.insert(negArr1, 1, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 2)\n",
      "(12500, 2)\n",
      "(12500, 2)\n",
      "(12500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(posArr.shape)\n",
    "print(negArr.shape)\n",
    "print(posArr1.shape)\n",
    "print(negArr1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation (4/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparing the data for ML model (1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame(data = posArr, columns = ['review','label'])\n",
    "df_neg = pd.DataFrame(data = negArr, columns = ['review','label'])\n",
    "df_pos1 = pd.DataFrame(data = posArr1, columns = ['review','label'])\n",
    "df_neg1 = pd.DataFrame(data = negArr1, columns = ['review','label'])\n",
    "df = df_pos.append(df_neg)\n",
    "df_test = df_pos1.append(df_neg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>24801</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review  label\n",
       "count                                               25000  25000\n",
       "unique                                              24801      2\n",
       "top     Loved today's show!!! It was a variety and not...      1\n",
       "freq                                                    5  12500"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>24904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>How has this piece of crap stayed on TV this l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review  label\n",
       "count                                               25000  25000\n",
       "unique                                              24904      2\n",
       "top     How has this piece of crap stayed on TV this l...      1\n",
       "freq                                                    3  12500"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cleaning and preprocessing of the text (2/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Nomalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all text to lowercase\n",
    "df['review'] = df['review'].str.lower()\n",
    "df_test['review'] = df_test['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove digits from text\n",
    "df['review'] = df['review'].str.replace('\\d+', '')\n",
    "df_test['review'] = df_test['review'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Special Characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(remove_special_characters)\n",
    "df_test['review'] = df_test['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df.label)\n",
    "y_test = lbl_enc.fit_transform(df_test.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "tokenizer=ToktokTokenizer()\n",
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "df['review']=df['review'].apply(remove_stopwords)\n",
    "df_test['review']=df_test['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "df['review']=df['review'].apply(simple_stemmer)\n",
    "df_test['review']=df_test['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Represent the text in numerical format (1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (25000, 3898050)\n",
      "BOW_cv_test: (25000, 3898050)\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "\n",
    "#transformed train reviews\n",
    "train_bow =cv.fit_transform(df['review'])\n",
    "\n",
    "#transformed test reviews\n",
    "test_bow =cv.transform(df_test['review'])\n",
    "\n",
    "print('BOW_cv_train:',train_bow.shape)\n",
    "print('BOW_cv_test:',test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf_train: (25000, 385818)\n",
      "tf_idf_test: (25000, 385818)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets\n",
    "tf_idf.fit(list(df['review']) + list(df_test['review']))\n",
    "train_tf_idf =  tf_idf.transform(df['review']) \n",
    "test_tf_idf = tf_idf.transform(df_test['review'])\n",
    "\n",
    "\n",
    "print('tf_idf_train:',train_tf_idf.shape)\n",
    "print('tf_idf_test:',test_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training ML classifier to predict the sentiment (1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55108"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "lr_bow = LogisticRegression(C=1.0)\n",
    "lr_bow.fit(train_bow, y)\n",
    "y_pred_bow = lr_bow.predict_proba(test_bow)\n",
    "\n",
    "lr_bow.score(test_bow,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87652"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "lr_tf_idf = LogisticRegression(C=1.0)\n",
    "lr_tf_idf.fit(train_tf_idf, y)\n",
    "y_pred_tf_idf = lr_tf_idf.predict_proba(test_tf_idf)\n",
    "\n",
    "lr_tf_idf.score(test_tf_idf,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the model extracting: precision, recall and f1 (1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "lr_bow_predict = lr_bow.predict(test_bow)\n",
    "print(lr_bow_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow_score : 0.55108\n"
     ]
    }
   ],
   "source": [
    "lr_bow_score=accuracy_score(y_test,lr_bow_predict)\n",
    "print(\"lr_bow_score :\",lr_bow_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.53      0.98      0.69     12500\n",
      "    Negative       0.86      0.12      0.21     12500\n",
      "\n",
      "    accuracy                           0.55     25000\n",
      "   macro avg       0.69      0.55      0.45     25000\n",
      "weighted avg       0.69      0.55      0.45     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_bow_report=classification_report(y_test,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1533 10967]\n",
      " [  256 12244]]\n"
     ]
    }
   ],
   "source": [
    "cm_bow=confusion_matrix(y_test,lr_bow_predict,labels=[1,0])\n",
    "print(cm_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "lr_tf_idf_predict = lr_tf_idf.predict(test_tf_idf)\n",
    "print(lr_tf_idf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_tf_idf_score : 0.87652\n"
     ]
    }
   ],
   "source": [
    "lr_tf_idf_score=accuracy_score(y_test,lr_tf_idf_predict)\n",
    "print(\"lr_tf_idf_score :\",lr_tf_idf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.86      0.88     12500\n",
      "    Negative       0.87      0.89      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_tf_idf_report=classification_report(y_test,lr_tf_idf_predict,target_names=['Positive','Negative'])\n",
    "print(lr_tf_idf_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11101  1399]\n",
      " [ 1688 10812]]\n"
     ]
    }
   ],
   "source": [
    "cm_tf_idf=confusion_matrix(y_test,lr_tf_idf_predict,labels=[1,0])\n",
    "print(cm_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction of model for one review (1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Very well made! The acting was great & the story was interesting throughout!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Can you extract general topics from this corpus of texts? (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Have you observed any limitations in this dataset? (1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset does not contain the ID of the individual who posted it. Some people might just post more bitter and negative reviews than other skewing the overall data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 What other techniques or methods that you haven’t used could you apply in any of the stages of the NLP pipeline to improve the performance of the model? (1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could potentially use different models from the Logistic regression.  \\\n",
    "I tried using lemmatization as seen in class but my laptop kept struggling and the results didn't improve.  \\\n",
    "A different stopword list can also be used. \\\n",
    "Noise could be reduced or features that barely apear can be removed.  \\\n",
    "N-grams and Bi grams could be used.  \\\n",
    "Furthermore, I could also use a larger training dataset by making the test one smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 In case you haven’t answered question 6, what technique could you use if you wanted to extract topics from the corpus of texts? (1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA could be used as it classifies text in the document to a particular topic.  \n",
    "\n",
    "Each review is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
